services:
  # PostgreSQL Database (optimized)
  # Pinned to 15-alpine: existing volume was initialized with PostgreSQL 15; 18+ also
  # changed data layout. Use 15 to match current data, or migrate with pg_upgrade.
  postgres:
    image: postgres:15-alpine
    container_name: ai_postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-ai_platform}
      POSTGRES_USER: ${POSTGRES_USER:-ai_admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-ai_password_2024}
      # Performance optimization
      POSTGRES_SHARED_BUFFERS: ${POSTGRES_SHARED_BUFFERS:-256MB}
      POSTGRES_EFFECTIVE_CACHE_SIZE: ${POSTGRES_EFFECTIVE_CACHE_SIZE:-1GB}
      POSTGRES_WORK_MEM: 4MB
      POSTGRES_MAINTENANCE_WORK_MEM: 64MB
      POSTGRES_MAX_CONNECTIONS: ${POSTGRES_MAX_CONNECTIONS:-100}
      # Security settings
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      ai_network:
        ipv4_address: 172.20.0.10
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-ai_admin} -d ${POSTGRES_DB:-ai_platform}"]
      interval: 180s  # Increased from 60s for energy efficiency
      timeout: 10s
      retries: 2
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    dns:
      - 127.0.0.11
      - 172.20.0.16


  # Redis Cache (optimized)
  redis:
    image: redis:alpine
    container_name: ai_redis
    restart: unless-stopped
    command: redis-server --requirepass ${REDIS_PASSWORD:-redis_password_2024} --maxmemory ${REDIS_MAXMEMORY:-256mb} --maxmemory-policy ${REDIS_MAXMEMORY_POLICY:-allkeys-lru} --save "" --appendonly no --tcp-keepalive 60
    volumes:
      - redis_data:/data
    networks:
      ai_network:
        ipv4_address: 172.20.0.11
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"${REDIS_PASSWORD:-redis_password_2024}\" ping | grep -q PONG || exit 1"]
      interval: 180s  # Increased from 60s for energy efficiency
      timeout: 10s
      retries: 2
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    dns:
      - 127.0.0.11
      - 172.20.0.16


  # Qdrant Vector Database (on-demand - Tier 2)
  # Only needed when using embeddings/RAG features
  qdrant:
    image: qdrant/qdrant:latest
    container_name: ai_qdrant
    restart: "no"  # on-demand (Tier 2) - start via dashboard/tu-vm.sh
    # keep internal only; access via internal URLs or add an authenticated proxy if needed
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      ai_network:
        ipv4_address: 172.20.0.12
    # Healthcheck disabled for on-demand service (Tier 2)
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    dns:
      - 127.0.0.11
      - 172.20.0.16


  # Ollama AI Models (on-demand - Tier 2)
  ollama:
    image: ollama/ollama:latest
    container_name: ai_ollama
    restart: "no"  # on-demand (Tier 2) - start via dashboard/tu-vm.sh
    # ports removed to enforce Nginx-only access (ollama.tu.local)
    volumes:
      - ollama_data:/root/.ollama
    networks:
      ai_network:
        ipv4_address: 172.20.0.13
    # No healthcheck for on-demand service
    deploy:
      resources:
        limits:
          memory: 2G  # Reduced from 3G for better idle efficiency
          cpus: '1.0'  # Reduced from 1.5 for better idle efficiency
        reservations:
          memory: 512M  # Reduced from 1G for better idle efficiency
          cpus: '0.25'  # Reduced from 0.5 for better idle efficiency
    dns:
      - 127.0.0.11
      - 172.20.0.16


  # Open WebUI (lightweight)
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: ai_openwebui
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    ports:
      # Bind to localhost for debugging only; primary access is via Nginx vhost (oweb.tu.local)
      - "127.0.0.1:8080:8080"
    environment:
      OLLAMA_BASE_URL: http://ollama:11434  # Will be dynamically checked
      WEBUI_SECRET_KEY: ${WEBUI_SECRET_KEY:-webui_secret_key_2024}
      WEBUI_JWT_SECRET_KEY: ${WEBUI_JWT_SECRET_KEY:-jwt_secret_key_2024}
      DATABASE_URL: postgresql://${POSTGRES_USER:-ai_admin}:${POSTGRES_PASSWORD:-ai_password_2024}@postgres:5432/${POSTGRES_DB:-ai_platform}
      QDRANT_URL: http://qdrant:6333
      REDIS_URL: redis://default:${REDIS_PASSWORD:-redis_password_2024}@redis:6379
      # HuggingFace download stability:
      # Open WebUI may use HuggingFace "xet" transfer which can get stuck in some environments.
      # Disabling it forces the classic download path.
      HF_HUB_DISABLE_XET: "1"
      # If HuggingFace downloads are blocked/slow (or upstream endpoints change),
      # Open WebUI can get stuck preloading the default embedding model and never bind to :8080.
      # Setting these empty makes startup resilient; you can re-enable embeddings later.
      RAG_EMBEDDING_MODEL: ""
      USE_EMBEDDING_MODEL_DOCKER: ""
      # Prevent startup hangs on pip installs for Tools/Functions dependencies
      # (Open WebUI runs pip installs on boot; keep timeouts low if outbound access is flaky)
      PIP_OPTIONS: "--default-timeout=10 --retries=1"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      # Mobile optimizations
      WEBUI_AUTH: "true"
      WEBUI_AUTH_SECRET: ${WEBUI_AUTH_SECRET:-auth_secret_2024}
      WEBUI_RATE_LIMIT: "50"
      WEBUI_RATE_LIMIT_WINDOW: "3600"
      # Reduce logging for battery
      LOG_LEVEL: "WARNING"
      # Disable telemetry
      ANONYMIZED_TELEMETRY: "false"
      DO_NOT_TRACK: "true"
      # Tika Configuration for PDF Processing
      CONTENT_EXTRACTION_ENGINE: "tika"
      TIKA_SERVER_URL: "http://ai_tika:9998"
      # Disable image extraction to prevent reshape errors
      PDF_EXTRACT_IMAGES: "false"
      # Enable Tika metadata extraction
      TIKA_METADATA_EXTRACTION: "true"
    volumes:
      # Shared volume for file sync between Open WebUI and MinIO
      - openwebui_files:/app/backend/data/uploads
      # Mount initialization script
      - ./scripts/init-openwebui.sh:/init-openwebui.sh:ro
    networks:
      ai_network:
        ipv4_address: 172.20.0.14
    healthcheck:
      # Use Open WebUI backend health endpoint (defined in open_webui/main.py)
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/health/db || exit 1"]
      # Open WebUI can block startup while installing tool/function deps; allow extra start time.
      interval: 180s  # Increased from 60s for energy efficiency
      timeout: 10s
      retries: 3
      start_period: 180s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    dns:
      - 127.0.0.11
      - 172.20.0.16

  

  # Apache Tika for robust content extraction with OCR support (on-demand - Tier 2)
  # Only needed when processing documents
  tika:
    image: apache/tika:latest-full  # Full version includes Tesseract OCR for image-based PDFs
    container_name: ai_tika
    restart: "no"  # on-demand (Tier 2) - start via dashboard/tu-vm.sh
    environment:
      - JAVA_TOOL_OPTIONS=-Xms256m -Xmx512m  # Increased memory for OCR processing
    command: ["--config", "/tika-config/tika-config.xml"]  # Custom config with extended timeout (15 minutes)
    volumes:
      - ./tika-config/tika-config.xml:/tika-config/tika-config.xml:ro
    networks:
      ai_network:
        ipv4_address: 172.20.0.20
    dns:
      - 127.0.0.11
      - 172.20.0.16

  # MinIO Object Storage with GUI (on-demand - Tier 2)
  minio:
    image: minio/minio:latest
    container_name: ai_minio
    restart: "no"  # Manual start only via dashboard
    ports:
      # Bind to localhost for tooling (mc/rclone/debug); primary access is via Nginx vhosts
      - "127.0.0.1:9000:9000"  # API
      - "127.0.0.1:9001:9001"  # Console GUI
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123456}
      MINIO_CONSOLE_ADDRESS: ":9001"
      MINIO_SERVER_URL: "https://api.minio.tu.local"
      MINIO_BROWSER_REDIRECT_URL: "https://minio.tu.local"
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    networks:
      ai_network:
        ipv4_address: 172.20.0.21
    # Healthcheck disabled for on-demand service (Tier 2)
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    dns:
      - 127.0.0.11
      - 172.20.0.16


  # n8n Workflow Automation (on-demand - Tier 2)
  n8n:
    image: n8nio/n8n:latest
    container_name: ai_n8n
    restart: "no"  # Manual start only via dashboard
    environment:
      N8N_BASIC_AUTH_ACTIVE: "true"
      N8N_BASIC_AUTH_USER: ${N8N_USER:-admin}
      N8N_BASIC_AUTH_PASSWORD: ${N8N_PASSWORD:-admin123}
      N8N_HOST: n8n.${DOMAIN}
      N8N_PORT: 5678
      N8N_PROTOCOL: https
      WEBHOOK_URL: https://n8n.${DOMAIN}/
      GENERIC_TIMEZONE: Europe/Zurich
      N8N_ENCRYPTION_KEY: ${N8N_ENCRYPTION_KEY:-1e2f3a4b5c6d7e8f9a0b1c2d3e4f5a6b}
      # PostgreSQL Database Configuration
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: ${POSTGRES_DB:-ai_platform}
      DB_POSTGRESDB_USER: ${POSTGRES_USER:-ai_admin}
      DB_POSTGRESDB_PASSWORD: ${POSTGRES_PASSWORD:-ai_password_2024}
      DB_POSTGRESDB_SCHEMA: n8n
      QDRANT_URL: http://qdrant:6333
      # Mobile optimizations
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: "true"
      N8N_RUNNERS_ENABLED: "true"
      # Reduce logging for battery
      N8N_LOG_LEVEL: "warn"
      # Disable telemetry
      N8N_DIAGNOSTICS_ENABLED: "false"
      N8N_TELEMETRY_ENABLED: "false"
      # Binary data storage in S3 (MinIO)
      N8N_BINARY_DATA_MODE: s3
      N8N_S3_ENDPOINT: ai_minio:9000
      N8N_S3_BUCKET: n8n-workflows
      N8N_S3_REGION: us-east-1
      N8N_S3_ACCESS_KEY: admin
      N8N_S3_ACCESS_SECRET: ${MINIO_ROOT_PASSWORD:-minio123456}
      N8N_S3_FORCE_PATH_STYLE: 'true'
    # Prefer access via Nginx (443). If you need direct host access, re-add 5678 mapping.
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      ai_network:
        ipv4_address: 172.20.0.15
    # Healthcheck disabled for on-demand service (Tier 2)
    # healthcheck:
    #   test: ["CMD-SHELL", "node -e \"require('http').get('http://127.0.0.1:5678/healthz', r => process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))\""]
    #   interval: 60s
    #   timeout: 10s
    #   retries: 2
    deploy:
      resources:
        limits:
          memory: 768M  # Reduced from 1G for efficiency
          cpus: '0.4'   # Reduced from 0.5 for efficiency
        reservations:
          memory: 512M
          cpus: '0.25'
    dns:
      - 127.0.0.11
      - 172.20.0.16


  # Pi-hole for Ad Blocking & DNS Security (Primary DNS on port 53)
  pihole:
    image: pihole/pihole:latest
    container_name: ai_pihole
    restart: unless-stopped
    dns:
      - 1.1.1.1
      - 9.9.9.9
    ports:
      - "53:53/udp"
      - "53:53/tcp"
      # Admin UI should be reachable only from localhost; external access via Nginx vhost (pihole.tu.local)
      - "127.0.0.1:8081:80"
    environment:
      TZ: 'Europe/Zurich'
      WEBPASSWORD: ${PIHOLE_PASSWORD:-'SwissPiHole2024!'}
      PIHOLE_DNS_: '1.1.1.1;9.9.9.9'
      DNSMASQ_USER: pihole
      VIRTUAL_HOST: "pihole.tu.local"
      PROXY_LOCATION: "pihole"
      ServerIP: "${HOST_IP}"
      DNSMASQ_LISTENING: all
      # Allow external queries
      DNSMASQ_INTERFACE: "eth0"
      DNSMASQ_BIND_INTERFACES: "false"
      DNSMASQ_LOCALISE_QUERIES: "false"
      # Block ads and trackers
      BLOCKING_ENABLED: "true"
      # Privacy features
      QUERY_LOGGING: "false"
      INSTALL_WEB_SERVER: "true"
      INSTALL_WEB_INTERFACE: "true"
      LIGHTTPD_ENABLED: "true"
      CACHE_SIZE: "10000"
    volumes:
      - pihole_data:/etc/pihole
      - pihole_dnsmasq:/etc/dnsmasq.d
      - ./pihole/01-custom.conf:/etc/dnsmasq.d/01-custom.conf:ro
    networks:
      ai_network:
        ipv4_address: 172.20.0.16
    healthcheck:
      test: ["CMD", "dig", "+norecurse", "+retry=0", "@127.0.0.1", "pi.hole"]
      interval: 180s  # Increased from 60s for energy efficiency
      timeout: 10s
      retries: 2
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'


  # Nginx Reverse Proxy (lightweight)
  nginx:
    image: nginx:alpine
    container_name: ai_nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/dynamic:/etc/nginx/dynamic:ro
      - ./nginx/html:/usr/share/nginx/html:ro
      - ./ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    ulimits:
      nofile:
        soft: 65535
        hard: 65535
    networks:
      ai_network:
        ipv4_address: 172.20.0.18
    depends_on:
      - helper_index
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 180s  # Increased from 60s for energy efficiency
      timeout: 10s
      retries: 2
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    dns:
      - 127.0.0.11
      - 172.20.0.16


  # Helper API for landing page (status only)
  helper_index:
    image: python:3-alpine
    container_name: ai_helper_index
    restart: unless-stopped
    command: sh -c "apk add --no-cache docker-cli docker-cli-compose curl && docker compose version && pip install --no-cache-dir flask requests requests-unixsocket && python -B -u /app/uploader.py"
    volumes:
      - ./helper:/app
      - ./docker-compose.yml:/app/docker-compose.yml:ro
      - .:/docker-project:ro  # Mount entire project for docker-compose path resolution
      - ./nginx/dynamic:/nginx-dynamic
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp:/tmp  # Share status file with tika-minio-processor for PDF processing notifications
    working_dir: /app
    environment:
      - HOST_IP=${HOST_IP}
      - CONTROL_TOKEN=${CONTROL_TOKEN}
    networks:
      ai_network:
        ipv4_address: 172.20.0.19
    extra_hosts:
      - "host.docker.internal:172.20.0.1"
    # no depends_on to avoid cycles with nginx
    dns:
      - 127.0.0.11
      - 172.20.0.16

  # Universal MinIO File Processor (on-demand - Tier 2)
  # Only needed when MinIO and Tika are running for document processing
  tika_minio_processor:
    build: ./tika-minio-processor
    container_name: tika_minio_processor
    environment:
      - TIKA_URL=http://ai_tika:9998
      - MINIO_ENDPOINT=ai_minio:9000
      - MINIO_ACCESS_KEY=admin
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-minio123456}
      - UPLOADS_BUCKET=tika-pipe
      - PROCESSED_BUCKET=tika-pipe
      - WATCH_BUCKETS=tika-pipe,n8n-workflows  # Watch multiple buckets (Open WebUI + n8n)
      - CHECK_INTERVAL=10  # Check for new files every 10 seconds
    volumes:
      - /tmp:/tmp  # Share status file with helper API for dashboard notifications
    networks:
      - ai_network
    restart: "no"  # on-demand (Tier 2) - start via dashboard/tu-vm.sh when needed
    depends_on:
      - tika
      # Note: MinIO is Tier 2 (on-demand), processor handles missing MinIO gracefully
    command: ["python", "universal_auto_processor.py"]
    # Healthcheck disabled for on-demand service (Tier 2)
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

volumes:
  openwebui_files:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  qdrant_data:
    driver: local
  ollama_data:
    driver: local
  minio_data:
    driver: local
  n8n_data:
    driver: local
  nginx_logs:
    driver: local
  pihole_data:
    driver: local
  pihole_dnsmasq:
    driver: local
  pdf-uploads:
    driver: local
  pdf-processed:
    driver: local

networks:
  ai_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1 